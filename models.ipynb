{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Rifky/Indobert-QA\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"Rifky/Indobert-QA\")\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "with open('data.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qa_pairs(data):\n",
    "    qa_pairs = []\n",
    "    for item in data:\n",
    "        context = item['judul'] + \" \" + \" \".join(item['keywords']) + f\" (ID: {item['infografis_id']})\"\n",
    "        # Question about the event\n",
    "        questions = [\n",
    "            f\"Apa yang terjadi pada tahun {item['tahun']}?\",\n",
    "            f\"Ada berapa infografis yang berkaitan dengan{item['judul']}?\",\n",
    "            f\"Infografis mana saja yang membahas tentang {item['keywords'][0]} di Jepara?\",\n",
    "            f\"Topik apa saja yang dibahas selain {item['keywords'][0]} dalam infografis dengan ID {item['id']}?\",\n",
    "            f\"Apa yang menjadi fokus utama infografis dengan ID {item['infografis_id']}?\",\n",
    "            f\"Apakah ada infografis lain yang berkaitan dengan topik yang sama dengan infografis ID {item['infografis_id']}?\"\n",
    "            f\"Carikan infografis tentang {item['keywords']}\"\n",
    "        ]\n",
    "        answers = [context] * len(questions) # Create a list of answers, one for each question\n",
    "\n",
    "        # Add each question-answer pair separately to qa_pairs\n",
    "        for question, answer in zip(questions, answers):\n",
    "            qa_pairs.append({'question': question, 'context': context, 'answer': answer})\n",
    "    return qa_pairs\n",
    "\n",
    "qa_pairs = create_qa_pairs(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(Dataset):\n",
    "    def __init__(self, encodings, answers):  # Add answers to the constructor\n",
    "        self.encodings = encodings\n",
    "        self.answers = answers\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        import torch\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        # Add answer information to the returned dictionary\n",
    "        item['start_positions'] = torch.tensor(self.answers[idx]['start_positions'])\n",
    "        item['end_positions'] = torch.tensor(self.answers[idx]['end_positions'])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "def find_answer_start_end(context, answer):\n",
    "    start_char = context.find(answer)\n",
    "    if start_char == -1:  # Answer not found in context\n",
    "        return 0, 0  # Handle cases where answer is not found\n",
    "    end_char = start_char + len(answer) - 1\n",
    "    return start_char, end_char\n",
    "\n",
    "def add_token_positions(encodings, answers, contexts):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i in range(len(answers)):\n",
    "        start_char, end_char = find_answer_start_end(contexts[i], answers[i])\n",
    "\n",
    "        # Convert char positions to token positions\n",
    "        start_token = encodings.char_to_token(i, start_char)\n",
    "        end_token = encodings.char_to_token(i, end_char)\n",
    "\n",
    "        # Handle cases where answer is not found within the context\n",
    "        if start_token is None:\n",
    "            start_token = 0  # CLS token\n",
    "        if end_token is None:\n",
    "            end_token = 0  # CLS token\n",
    "\n",
    "        start_positions.append(start_token)\n",
    "        end_positions.append(end_token)\n",
    "\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "\n",
    "# Split data into train and evaluation sets (adjust split ratio as needed)\n",
    "split_ratio = 0.8  # Use 80% for training, 20% for evaluation\n",
    "split_index = int(len(qa_pairs) * split_ratio)\n",
    "train_qa_pairs = qa_pairs[:split_index]\n",
    "eval_qa_pairs = qa_pairs[split_index:]\n",
    "\n",
    "# Tokenize the train and evaluation datasets separately\n",
    "train_encodings = tokenizer(\n",
    "    [pair['question'] for pair in train_qa_pairs],\n",
    "    [pair['context'] for pair in train_qa_pairs],\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=384,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "eval_encodings = tokenizer(\n",
    "    [pair['question'] for pair in eval_qa_pairs],\n",
    "    [pair['context'] for pair in eval_qa_pairs],\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=384,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Add token positions for train and evaluation datasets\n",
    "add_token_positions(train_encodings, [pair['answer'] for pair in train_qa_pairs], [pair['context'] for pair in train_qa_pairs])\n",
    "add_token_positions(eval_encodings, [pair['answer'] for pair in eval_qa_pairs], [pair['context'] for pair in eval_qa_pairs])\n",
    "\n",
    "# Create train and evaluation datasets\n",
    "train_dataset = QADataset(train_encodings, [{'start_positions': train_encodings['start_positions'][i], 'end_positions': train_encodings['end_positions'][i]} for i in range(len(train_encodings['start_positions']))])\n",
    "eval_dataset = QADataset(eval_encodings, [{'start_positions': eval_encodings['start_positions'][i], 'end_positions': eval_encodings['end_positions'][i]} for i in range(len(eval_encodings['start_positions']))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\diaje\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f94e251b1c064f08b33f66012983ecbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/159 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    dataloader_num_workers=3,\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset, # Use the dataset objects\n",
    "    eval_dataset=eval_dataset,   # Use the dataset objects\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
